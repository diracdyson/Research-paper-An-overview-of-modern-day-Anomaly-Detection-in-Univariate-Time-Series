{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler as sc\n",
    "returnsant = pd.read_csv('returnsant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoderr(see):\n",
    "     if see ==9:\n",
    "        return keras.Sequential([tf.keras.layers.Dense(6,kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=see),\n",
    "                                                 trainable=True),\n",
    "                           tf.keras.layers.Dense(4\n",
    "                           ,kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=see+1),trainable=True),\n",
    "                           tf.keras.layers.Dense(2\n",
    "                           ,kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=see+2),trainable=True),\n",
    "                           tf.keras.layers.Dense(1,activation=\"relu\"\n",
    "                           ,kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=see+2),trainable=True)])\n",
    "     else:\n",
    "        return keras.Sequential([tf.keras.layers.Dense(6\n",
    "                                                 ,kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=see),trainable=False),\n",
    "                           tf.keras.layers.Dense(4\n",
    "                           ,kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=see+1),trainable=False),\n",
    "                           tf.keras.layers.Dense(2\n",
    "                           ,kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=see+2),trainable=False),\n",
    "                           tf.keras.layers.Dense(1,activation=\"relu\"\n",
    "                           ,kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=see+2),trainable=False)],) # extra comma for tuple\n",
    "decoderr= keras.Sequential([tf.keras.layers.Dense(1),\n",
    "                            tf.keras.layers.Dense(2),\n",
    "                             tf.keras.layers.Dense(4),\n",
    "                           tf.keras.layers.Dense(6,activation=\"relu\")],)\n",
    "encoderrs=[]\n",
    "for i in range(0,10):\n",
    "    encoderrs.append(encoderr(i))\n",
    "encoderrs=np.array(encoderrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa7c469c510>,\n",
       "       <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa7c46a7490>,\n",
       "       <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa7c46af410>,\n",
       "       <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa7c46b6390>,\n",
       "       <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa7c46bc310>,\n",
       "       <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa7c46272d0>,\n",
       "       <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa7c462f250>,\n",
       "       <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa7c46351d0>,\n",
       "       <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa7c463f150>,\n",
       "       <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa7c463ff90>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoderrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create error vector to weight each DATA set fed into each respective encoderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostedAE(keras.Model):\n",
    "    def __init__(self, encoder,decoder):\n",
    "        super(BoostedAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def compile(self):\n",
    "        super(BoostedAE, self).compile()\n",
    "\n",
    "\n",
    "    def fit(self,x,y,batch_size,epochs):\n",
    "        \n",
    "        tf.keras.backend.set_floatx('float64')\n",
    "        loss_fn=keras.losses.MeanSquaredError()\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.00001)\n",
    "        \n",
    "        train_acc_metric=keras.metrics.MeanSquaredError()\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "        train_dataset = train_dataset.batch(batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "            start_time = time.time()\n",
    "            \n",
    "    # Iterate over the batches of the dataset.\n",
    "            \n",
    "            #intialize with equal voting 1/N wewights \n",
    "            ws=[0.1]*10\n",
    "            for step, (x_batch_train,y_batch_train ) in enumerate(train_dataset):\n",
    "                boost=[]\n",
    "                \n",
    "                #print(x_batch_train)        \n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    for e in range(0,10):\n",
    "                        #print(ws[e])\n",
    "                        ok = self.encoder[e](ws[e]*x_batch_train)\n",
    "                        ok=(tf.reshape(ok,(x_batch_train.shape[0],1)))\n",
    "                        ok=tf.cast(ok,tf.float64)\n",
    "                        boost.append(ok)\n",
    "                        ws[e]=(np.sum(ok-y_batch_train))**2\n",
    "                        #print(ws[e])\n",
    "                      # print(ok)`\n",
    "                        \n",
    "                       # print(y_batch_train)\n",
    "                        \n",
    "                        #print(ok)\n",
    "                        \n",
    "                       # print((ok-y_batch_train)**2)\n",
    "                    \n",
    "                        # assert that the ws sum to 1 and hence wieghted by sum\n",
    "                    if np.sum(ws)!=1:    \n",
    "                        ws=np.array(ws)\n",
    "                        ws=ws/np.sum(ws)\n",
    "                    else:\n",
    "                        pass\n",
    "                    #print(ws)\n",
    "                    #print(np.sum(ws))\n",
    "                    #print(ws)\n",
    "                   # print('boost{}'.format(boost.shape()))\n",
    "                    #implement cross val aspect of training loop\n",
    "                    boost=np.array(boost)\n",
    "                    boost.shape = ((x_batch_train.shape[0],10))\n",
    "                    # insert weighted function  on each x_i\n",
    "                    \n",
    "                    newx=np.average(boost,axis=1)\n",
    "                    newx.shape=((x_batch_train.shape[0],1))\n",
    "                    \n",
    "                    predictions = self.decoder(newx)\n",
    "                    \n",
    "                    loss_value = loss_fn(y_batch_train, predictions)\n",
    "                    loss_value += sum(self.losses)\n",
    "                    \n",
    "                    if step % 20 == 0:\n",
    "                    \n",
    "                        print(\n",
    "                        \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                        % (step, float(loss_value))\n",
    "                         ) \n",
    "                    \n",
    "                        print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "                    \n",
    "                grads = tape.gradient(loss_value, self.trainable_weights )\n",
    "                \n",
    "\n",
    "                optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "  \n",
    "                train_acc_metric.update_state(y_batch_train,predictions)\n",
    "            \n",
    "            train_acc=train_acc_metric.result()\n",
    "            print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "            train_acc_metric.reset_states()\n",
    "            print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "        return {'metric_a':loss_value}\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bae=BoostedAE(encoderrs,decoderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bae.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.0005\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.74s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.0005\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.28s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.0005\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.17s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.0005\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.13s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.0005\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.21s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.0005\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.26s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.0005\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.32s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.19s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.29s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.54s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.33s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.44s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.20s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.14s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.12s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.14s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.18s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.16s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.16s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.44s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.30s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.18s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.18s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.14s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.20s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.15s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.57s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.21s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.15s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.15s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.65s\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.24s\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.16s\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.18s\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.15s\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.15s\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.16s\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.14s\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.15s\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.17s\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.16s\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.14s\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.12s\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.17s\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.14s\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.12s\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.35s\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.24s\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.19s\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.15s\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.41s\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.60s\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.84s\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.76s\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.73s\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.63s\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.46s\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.63s\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.51s\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.25s\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.21s\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.26s\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.43s\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 3.89s\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 4.43s\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 4.41s\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 4.05s\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 3.90s\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 3.91s\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 5.44s\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 8.84s\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 7.66s\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 3.73s\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 8.89s\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 5.29s\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 3.07s\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.72s\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.83s\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.75s\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.65s\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.81s\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.79s\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.69s\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.53s\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.90s\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.88s\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.71s\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.63s\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.67s\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.60s\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.55s\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.83s\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.74s\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.67s\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.89s\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.72s\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.58s\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.62s\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.64s\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 0.0004\n",
      "Seen so far: 20 samples\n",
      "Training acc over epoch: 0.0009\n",
      "Time taken: 2.60s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric_a': <tf.Tensor: id=394218, shape=(), dtype=float32, numpy=0.0020153874>}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bae.fit(returnsant['Hedge Fund'].values.reshape(-1,1),returnsant['Hedge Fund'].values.reshape(-1,1),batch_size=20,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
